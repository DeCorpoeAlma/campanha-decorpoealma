from flask import Flask, request, jsonify
from flask_cors import CORS
import requests
import json
import os
from datetime import datetime
import logging
from dotenv import load_dotenv

load_dotenv() # Carrega as vari√°veis do arquivo .env

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Configura√ß√µes da Hugging Face
HUGGING_FACE_API_URL = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium"
# Alternativas gratuitas:
# "microsoft/DialoGPT-large" - Melhor para conversa√ß√£o
# "facebook/blenderbot-400M-distill" - Bom para chat geral
# "google/flan-t5-base" - Bom para Q&A

# Voc√™ precisa criar uma conta gratuita em https://huggingface.co e obter um token
HUGGING_FACE_TOKEN = os.getenv('HUGGING_FACE_TOKEN', 'seu_token_aqui')

# Contexto do seu site/neg√≥cio - adapte conforme necess√°rio
CONTEXT_INFO = """
Voc√™ √© um assistente inteligente que ajuda usu√°rios com informa√ß√µes sobre nossa empresa.
Nossa empresa oferece servi√ßos de tecnologia e desenvolvimento web.
Sempre seja √∫til, educado e profissional em suas respostas.
Se n√£o souber algo espec√≠fico, seja honesto e ofere√ßa ajuda alternativa.
"""

class LLMChatbot:
    def __init__(self):
        self.conversation_history = {}
        self.headers = {
            "Authorization": f"Bearer {HUGGING_FACE_TOKEN}",
            "Content-Type": "application/json"
        }

    def get_session_id(self, request):
        """Gera um ID de sess√£o √∫nico para manter o contexto da conversa"""
        return request.remote_addr + str(hash(request.headers.get('User-Agent', '')))

    def clean_response(self, response_text):
        """Limpa e formata a resposta da LLM"""
        if not response_text:
            return "Desculpe, n√£o consegui gerar uma resposta no momento."

        # Remove tokens especiais e limpa a resposta
        cleaned = response_text.strip()
        if cleaned.startswith('<|endoftext|>'):
            cleaned = cleaned.replace('<|endoftext|>', '').strip()

        return cleaned if cleaned else "Posso ajud√°-lo com algo mais espec√≠fico?"

    def query_huggingface(self, prompt, max_length=150):
        """Faz consulta √† API da Hugging Face"""
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_length": max_length,
                "temperature": 0.7,
                "do_sample": True,
                "pad_token_id": 50256
            },
            "options": {
                "wait_for_model": True
            }
        }

        try:
            response = requests.post(
                HUGGING_FACE_API_URL,
                headers=self.headers,
                json=payload,
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                if isinstance(result, list) and len(result) > 0:
                    generated_text = result[0].get('generated_text', '')
                    # Remove o prompt original da resposta
                    if generated_text.startswith(prompt):
                        generated_text = generated_text[len(prompt):].strip()
                    return self.clean_response(generated_text)
                else:
                    return "N√£o consegui gerar uma resposta adequada."
            else:
                logger.error(f"Erro na API Hugging Face: {response.status_code} - {response.text}")
                return "Desculpe, estou tendo dificuldades t√©cnicas no momento."

        except requests.exceptions.Timeout:
            return "A resposta est√° demorando mais que o esperado. Tente novamente."
        except Exception as e:
            logger.error(f"Erro ao consultar LLM: {str(e)}")
            return "Ocorreu um erro interno. Tente novamente em alguns instantes."

    def generate_response(self, user_message, session_id):
        """Gera resposta usando a LLM com contexto"""

        # Recupera hist√≥rico da conversa
        if session_id not in self.conversation_history:
            self.conversation_history[session_id] = []

        history = self.conversation_history[session_id]

    def read_file_content(self, filepath):
        """L√™ o conte√∫do de um arquivo."""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            logger.error(f"Arquivo n√£o encontrado: {filepath}")
            return ""
        except Exception as e:
            logger.error(f"Erro ao ler arquivo {filepath}: {str(e)}")
            return ""

    def extract_text_from_frontend(self, filepath):
        """L√™ um arquivo do frontend e tenta extrair strings de texto."""
        content = self.read_file_content(filepath)
        if not content:
            return ""

        # Implementa√ß√£o simples: extrair strings entre aspas
        # Isso pode n√£o ser perfeito para todos os casos e pode incluir chaves de tradu√ß√£o, etc.
        # Uma an√°lise de AST seria mais robusta, mas mais complexa.
        import re
        strings = re.findall(r'"([^"\\]*(?:\\.[^"\\]*)*)"|\'([^\'\\]*(?:\\.[^\'\\]*)*)\'', content)
        extracted_text = " ".join([s[0] or s[1] for s in strings])

        # Remover m√∫ltiplos espa√ßos em branco e quebras de linha
        cleaned_text = re.sub(r'\s+', ' ', extracted_text).strip()

        return cleaned_text

    def generate_response(self, user_message, session_id):
        """Gera resposta usando a LLM com contexto"""

        # Recupera hist√≥rico da conversa
        if session_id not in self.conversation_history:
            self.conversation_history[session_id] = []

        history = self.conversation_history[session_id]

        # Carregar conte√∫do do site como contexto
        site_context = ""
        
        # Caminhos dos arquivos do frontend (ajuste conforme a estrutura do seu projeto)
        frontend_files = [
            '../src/components/Program.tsx',
            '../src/components/Team.tsx',
            '../src/components/CandidateSection.tsx',
            '../src/data/eventsData.ts',
            # Adicione outros arquivos relevantes aqui
        ]

        for filepath in frontend_files:
            extracted_text = self.extract_text_from_frontend(filepath)
            if extracted_text:
                site_context += f"Conte√∫do de {filepath}:\n{extracted_text}\n\n"

        # Constr√≥i o prompt com contexto
        prompt = CONTEXT_INFO + "\n\n" + site_context + "\n\n"

        # Adiciona hist√≥rico recente (√∫ltimas 3 intera√ß√µes)
        recent_history = history[-6:] if len(history) > 6 else history
        for msg in recent_history:
            prompt += f"Usu√°rio: {msg['user']}\nAssistente: {msg['bot']}\n"

        prompt += f"Usu√°rio: {user_message}\nAssistente:"

        # Gera resposta
        response = self.query_huggingface(prompt)

        # Salva no hist√≥rico
        history.append({
            'user': user_message,
            'bot': response,
            'timestamp': datetime.now().isoformat()
        })

        # Limita o hist√≥rico para n√£o ficar muito grande
        if len(history) > 20:
            self.conversation_history[session_id] = history[-20:]

        return response

# Instanciar o chatbot
chatbot = LLMChatbot()

@app.route('/chat', methods=['POST'])
def chat():
    try:
        data = request.get_json()

        if not data or 'message' not in data:
            return jsonify({"error": "Mensagem n√£o fornecida"}), 400

        user_message = data.get('message', '').strip()

        if not user_message:
            return jsonify({"error": "Mensagem vazia"}), 400

        # Obt√©m ID da sess√£o para manter contexto
        session_id = chatbot.get_session_id(request)

        # Gera resposta usando a LLM
        bot_response = chatbot.generate_response(user_message, session_id)

        logger.info(f"Pergunta: {user_message[:50]}... | Resposta: {bot_response[:50]}...")

        return jsonify({
            "response": bot_response,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"Erro no endpoint /chat: {str(e)}")
        return jsonify({"error": "Erro interno do servidor"}), 500

@app.route('/health', methods=['GET'])
def health_check():
    """Endpoint para verificar se o servi√ßo est√° funcionando"""
    return jsonify({
        "status": "online",
        "timestamp": datetime.now().isoformat(),
        "model": "Hugging Face LLM"
    })

@app.route('/clear_history', methods=['POST'])
def clear_history():
    """Limpa o hist√≥rico de conversa de uma sess√£o"""
    session_id = chatbot.get_session_id(request)
    if session_id in chatbot.conversation_history:
        del chatbot.conversation_history[session_id]
    return jsonify({"message": "Hist√≥rico limpo com sucesso"})

if __name__ == '__main__':
    # Verificar se o token foi configurado
    if HUGGING_FACE_TOKEN == 'seu_token_aqui':
        print("‚ö†Ô∏è  ATEN√á√ÉO: Configure seu token da Hugging Face!")
        print("1. V√° para https://huggingface.co")
        print("2. Crie uma conta gratuita")
        print("3. V√° em Settings > Access Tokens")
        print("4. Crie um novo token")
        print("5. Configure a vari√°vel de ambiente: export HUGGING_FACE_TOKEN='seu_token'")
        print("6. Ou substitua 'seu_token_aqui' no c√≥digo")

    print("ü§ñ Iniciando chatbot com LLM gratuita...")
    print("üì° API dispon√≠vel em: http://localhost:6000")
    print("üîç Health check: http://localhost:6000/health")

    app.run(debug=True, port=6000, host='0.0.0.0')