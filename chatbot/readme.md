# OpenRouter Chatbot Backend

Backend completo em Python para chatbot usando OpenRouter API com FastAPI.

## üì¶ Instala√ß√£o

### 1. Crie um ambiente virtual:
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows
```

### 2. Instale as depend√™ncias:
```bash
pip install fastapi uvicorn httpx pydantic python-multipart
```

### 3. Execute o servidor:
```bash
python main.py
```

O servidor estar√° dispon√≠vel em: `http://localhost:8000`

## üîß Configura√ß√£o

### Obter Chave da API OpenRouter:
1. Visite [openrouter.ai](https://openrouter.ai)
2. Crie uma conta gratuita
3. V√° em "Keys" e gere uma nova chave da API
4. Use a chave no header Authorization: `Bearer sua_chave_aqui`

## üìö Documenta√ß√£o da API

Acesse a documenta√ß√£o interativa em:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

## üöÄ Endpoints Principais

### 1. Chat Principal
```http
POST /chat
Authorization: Bearer sua_chave_openrouter
Content-Type: application/json

{
  "message": "Ol√°, como voc√™ est√°?",
  "model": "anthropic/claude-3.5-sonnet",
  "temperature": 0.7,
  "max_tokens": 1000,
  "session_id": "optional-session-id"
}
```

**Resposta:**
```json
{
  "response": "Ol√°! Estou bem, obrigado por perguntar...",
  "model_used": "anthropic/claude-3.5-sonnet",
  "session_id": "uuid-da-sessao",
  "tokens_used": 45,
  "timestamp": "2025-06-13T10:30:00"
}
```

### 2. Listar Modelos Dispon√≠veis
```http
GET /models
```

### 3. Informa√ß√µes da Sess√£o
```http
GET /sessions/{session_id}
```

### 4. Limpar Sess√£o
```http
DELETE /sessions/{session_id}
```

## üêç Exemplo de Cliente Python

```python
import httpx
import asyncio

class OpenRouterClient:
    def __init__(self, api_key: str, base_url: str = "http://localhost:8000"):
        self.api_key = api_key
        self.base_url = base_url
        self.session_id = None
    
    async def chat(self, message: str, model: str = "anthropic/claude-3.5-sonnet"):
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.base_url}/chat",
                headers={"Authorization": f"Bearer {self.api_key}"},
                json={
                    "message": message,
                    "model": model,
                    "session_id": self.session_id
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                self.session_id = data["session_id"]
                return data["response"]
            else:
                return f"Erro: {response.status_code} - {response.text}"

# Uso
async def main():
    client = OpenRouterClient("sua_chave_openrouter")
    
    response = await client.chat("Ol√°, explique o que √© machine learning")
    print(response)
    
    # Continuar conversa (mant√©m contexto)
    response = await client.chat("Pode dar um exemplo pr√°tico?")
    print(response)

# Executar
asyncio.run(main())
```

## üåê Exemplo de Cliente JavaScript/Frontend

```javascript
class OpenRouterAPI {
    constructor(apiKey, baseUrl = 'http://localhost:8000') {
        this.apiKey = apiKey;
        this.baseUrl = baseUrl;
        this.sessionId = null;
    }

    async chat(message, model = 'anthropic/claude-3.5-sonnet') {
        try {
            const response = await fetch(`${this.baseUrl}/chat`, {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${this.apiKey}`,
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    message: message,
                    model: model,
                    session_id: this.sessionId
                })
            });

            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }

            const data = await response.json();
            this.sessionId = data.session_id;
            return data.response;

        } catch (error) {
            console.error('Erro no chat:', error);
            throw error;
        }
    }

    async getModels() {
        const response = await fetch(`${this.baseUrl}/models`);
        return await response.json();
    }
}

// Uso
const api = new OpenRouterAPI('sua_chave_openrouter');

api.chat('Ol√°, como voc√™ est√°?')
    .then(response => console.log(response))
    .catch(error => console.error(error));
```

## üîí Recursos de Seguran√ßa

- **Autentica√ß√£o por Bearer Token**: Cada requisi√ß√£o precisa da chave da API
- **Valida√ß√£o de entrada**: Todos os dados s√£o validados com Pydantic
- **Rate limiting**: Implementado pelo pr√≥prio OpenRouter
- **CORS configurado**: Para permitir requisi√ß√µes do frontend
- **Logs detalhados**: Para monitoramento e debugging

## üìä Gerenciamento de Sess√µes

- **Sess√µes autom√°ticas**: Cada conversa tem um ID √∫nico
- **Contexto persistente**: Mant√©m hist√≥rico da conversa
- **Limpeza autom√°tica**: Remove sess√µes antigas automaticamente
- **Limite de mem√≥ria**: Evita ac√∫mulo excessivo de dados

## üîß Configura√ß√µes Avan√ßadas

### Vari√°veis de Ambiente (opcional)
```bash
# .env
OPENROUTER_DEFAULT_MODEL=anthropic/claude-3.5-sonnet
MAX_TOKENS_DEFAULT=1000
SESSION_CLEANUP_HOURS=24
LOG_LEVEL=INFO
```

### Docker (opcional)
```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY main.py .

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

## üöÄ Deploy em Produ√ß√£o

### 1. Heroku
```bash
# Procfile
web: uvicorn main:app --host 0.0.0.0 --port $PORT
```

### 2. Railway/Render
- Configure a vari√°vel de ambiente `PORT`
- Use `uvicorn main:app --host 0.0.0.0 --port $PORT`

### 3. VPS/Servidor Pr√≥prio
```bash
# Com Gunicorn para produ√ß√£o
pip install gunicorn
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
```

## üìà Monitoramento

O backend inclui:
- Logs estruturados
- Endpoint de health check (`/health`)
- M√©tricas de uso por sess√£o
- Tratamento de erros robusto

## ü§ù Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudan√ßas
4. Push para a branch
5. Abra um Pull Request

## üìÑ Licen√ßa

MIT License - veja o arquivo LICENSE para detalhes.